{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import h5py\n",
    "import tensortools as tt # toolbox for TCA\n",
    "import os\n",
    "import numpy as np\n",
    "import importlib as imp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import load_preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate a file to analyze\n",
    "fname = 'VJ_OFCVTA_7_260_D6'\n",
    "fdir = 'C:\\\\2pData\\\\Vijay data\\\\VJ_OFCVTA_7_D8_trained\\\\'\n",
    "sima_h5_path = os.path.join(fdir, fname + '_sima_mc.h5')\n",
    "\n",
    "# set the sampling rate\n",
    "fs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial windowing \n",
    "trial_start_end_seconds = np.array([-1, 3]) # trial windowing in seconds relative to ttl-onset/trial-onset\n",
    "conditions = ['minus', 'plus_rewarded']\n",
    "\n",
    "# if helper scripts have been updated, can refresh them with this line\n",
    "imp.reload(utils)\n",
    "imp.reload(load_preprocess_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract trial data into xarray\n",
    "\n",
    "num_avg_groups = 5.0 \n",
    "\"\"\" number of segments to split trials over. Ie. Because single trial plots in state space is noisy, \n",
    "    let's break the trials up into groups and average to get less noisier signal.\n",
    "\"\"\" \n",
    "\n",
    "data_dict = load_preprocess_data.load(fname, fdir, fs, trial_start_end_seconds, conditions, num_avg_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make data input\n",
    "\n",
    "condition = 'plus_rewarded'\n",
    "\n",
    "X = data_dict['all_cond']['flattenpix'] # data_dict[condition]['xarr_flatten_xy'].data\n",
    "#dims are (trial, yx_pix, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ranks = 30\n",
    "\n",
    "print('Run the same data through TCA twice to test the consistency of their models')\n",
    "\n",
    "U = tt.ncp_hals(X, rank=num_ranks, verbose=False) # CP decomposition by classic alternating least squares (ALS).\n",
    "# The `rank` sets the number of components to be computed.\n",
    "# output are factor matrices of the fitted results\n",
    "# objective function is the frobenius norm\n",
    "V = tt.ncp_hals(X, rank=num_ranks, verbose=False)\n",
    "\n",
    "# Compare the low-dimensional factors from the two fits.\n",
    "fig, _, _ = tt.plot_factors(U.factors)\n",
    "tt.plot_factors(V.factors, fig=fig)\n",
    "\n",
    "print('Align the two runs/iterations and plot')\n",
    "\n",
    "# Align the two fits and print a similarity score.\n",
    "sim = tt.kruskal_align(U.factors, V.factors, permute_U=True, permute_V=True)\n",
    "print(sim)\n",
    "\n",
    "# Plot the results again to see alignment.\n",
    "fig, ax, po = tt.plot_factors(U.factors)\n",
    "tt.plot_factors(V.factors, fig=fig)\n",
    "\n",
    "# Show plots.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Error and Model Similarity (within data set) As Function of rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "# Fit ensembles of tensor decompositions.\n",
    "\n",
    "methods = (\n",
    "\n",
    "  'cp_als',    # fits unconstrained tensor decomposition.\n",
    "\n",
    "  'ncp_bcd',   # fits nonnegative tensor decomposition.\n",
    "\n",
    "  'ncp_hals',  # fits nonnegative tensor decomposition.\n",
    "\n",
    ")\n",
    "\n",
    "ensembles = {}\n",
    "\n",
    "\"\"\"\n",
    "    fit_option arguments:\n",
    "    \n",
    "        tol : float, optional (default ``tol=1E-5``)\n",
    "        Stopping tolerance for reconstruction error.\n",
    "\n",
    "        max_iter : integer, optional (default ``max_iter = 500``)\n",
    "        Maximum number of iterations to perform before exiting.\n",
    "    \n",
    "        min_iter : integer, optional (default ``min_iter = 1``)\n",
    "        Minimum number of iterations to perform before exiting.\n",
    "\n",
    "        max_time : integer, optional (default ``max_time = np.inf``)\n",
    "        Maximum computational time before exiting.\n",
    "\n",
    "        verbose : bool ``{'True', 'False'}``, optional (default ``verbose=True``)\n",
    "        Display progress.\n",
    "\"\"\"\n",
    "\n",
    "for m in methods:\n",
    "\n",
    "    ensembles[m] = tt.Ensemble(fit_method=m, fit_options=dict(tol=1e-3)) # tolerance: percent error threshold to terminate\n",
    "\n",
    "    ensembles[m].fit(X, ranks=range(1, num_ranks), replicates=3)\n",
    "\n",
    "# Plotting options for the unconstrained and nonnegative models.\n",
    "\n",
    "plot_options = {\n",
    "\n",
    "  'cp_als': {\n",
    "\n",
    "    'line_kw': {\n",
    "\n",
    "      'color': 'black',\n",
    "      'label': 'cp_als',\n",
    "\n",
    "    },\n",
    "\n",
    "    'scatter_kw': {\n",
    "\n",
    "      'color': 'black',\n",
    "\n",
    "    },\n",
    "  },\n",
    "\n",
    "  'ncp_hals': {\n",
    "\n",
    "    'line_kw': {\n",
    "\n",
    "      'color': 'blue',\n",
    "      'alpha': 0.5,\n",
    "      'label': 'ncp_hals',\n",
    "\n",
    "    },\n",
    "\n",
    "    'scatter_kw': {\n",
    "\n",
    "      'color': 'blue',\n",
    "      'alpha': 0.5,\n",
    "\n",
    "    },\n",
    "  },\n",
    "\n",
    "  'ncp_bcd': {\n",
    "\n",
    "    'line_kw': {\n",
    "\n",
    "      'color': 'red',\n",
    "      'alpha': 0.5,\n",
    "      'label': 'ncp_bcd',\n",
    "\n",
    "    },\n",
    "\n",
    "    'scatter_kw': {\n",
    "\n",
    "      'color': 'red',\n",
    "      'alpha': 0.5,\n",
    "\n",
    "    },\n",
    "  },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Plot similarity and error plots.\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for m in methods:\n",
    "\n",
    "    tt.plot_objective(ensembles[m], **plot_options[m])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for m in methods:\n",
    "\n",
    "    tt.plot_similarity(ensembles[m], **plot_options[m])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V.factors[time_factor][:,component]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# container to store data relevant to the 3d plot\n",
    "s_space_dict = {}\n",
    "s_space_dict['line_cmaps'] = ['autumn','winter']\n",
    "\n",
    "# determine alpha for each trial (encoding time block)\n",
    "trial_group_alphas = np.linspace(0.3, 1, num_avg_groups)\n",
    "\n",
    "# loop through conditions\n",
    "for idx_condition, condition in enumerate(conditions):\n",
    "\n",
    "    # set up variables for this condition\n",
    "    s_space_dict[condition] = {} # sub-dict for condition-specific data \n",
    "    n = data_dict[condition]['num_samples'] # number of data points\n",
    "    cmap_lc = s_space_dict['line_cmaps'][idx_condition] # grab this condition's line cmap\n",
    "    \n",
    "    #set x,y,z, time data\n",
    "    x = data_dict[condition]['Xt'][:,0]\n",
    "    y = data_dict[condition]['Xt'][:,1]\n",
    "    z = data_dict[condition]['Xt'][:,2]\n",
    "    svec = np.arange(0,n) # sample vector; important for encoding color as time\n",
    "    # USER DEFINE: which dimension to encode color; can be x, y, z, svec\n",
    "    color_encode = svec \n",
    "    \n",
    "    # update x,y,z limits based on this condition's data\n",
    "    if idx_condition == 0:\n",
    "        xlim = [np.min(x), np.max(x)]; ylim = [np.min(y), np.max(y)]; zlim = [np.min(z), np.max(z)]\n",
    "    else:\n",
    "        xlim = update_lims([np.min(x), np.max(x)], xlim); \n",
    "        ylim = update_lims([np.min(y), np.max(y)], ylim); \n",
    "        zlim = update_lims([np.min(z), np.max(z)], zlim);\n",
    "    \n",
    "    ### Create line segment objects for ALL TRIAL-AVGED DATA ###\n",
    "    s_space_dict[condition]['line_collect'] = make_line_collection(x, y, z, color_encode, cmap_lc)\n",
    "    \n",
    "    ### Create line segment objects for TRIAL-BLOCKED/GROUPED DATA ###\n",
    "    s_space_dict[condition]['line_collect_trial'] = {}\n",
    "    for idx, trial in enumerate(data_dict[condition]['Xt_trial']):\n",
    "        \n",
    "        # make the line segment object for this trial group\n",
    "        s_space_dict[condition]['line_collect_trial'][idx] = make_line_collection(trial[:,0], trial[:,1], trial[:,2], \n",
    "                                                                                  color_encode, \n",
    "                                                                                  cmap_lc, \n",
    "                                                                                  trial = True, \n",
    "                                                                                  alpha = trial_group_alphas[idx])\n",
    "\n",
    "        # update x,y,z limits based on this \"trial's\" data\n",
    "        xlim = update_lims([np.min(trial[:,0]), np.max(trial[:,0])], xlim); \n",
    "        ylim = update_lims([np.min(trial[:,1]), np.max(trial[:,1])], ylim);\n",
    "        zlim = update_lims([np.min(trial[:,2]), np.max(trial[:,2])], zlim);\n",
    "        \n",
    "# create plot and set attributes\n",
    "fig = plt.figure(figsize = (9,7))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_xlim(xlim); ax.set_ylim(ylim); ax.set_zlim(zlim)\n",
    "plt.title('PCA State Space')\n",
    "ax.set_xlabel('PC0', fontsize = 20); ax.set_ylabel('PC1', fontsize = 20); ax.set_zlabel('PC2', fontsize = 20);\n",
    "\n",
    "# plot the line segments\n",
    "for condition in conditions:\n",
    "    \n",
    "    # for all trial-avged data\n",
    "    ax.add_collection3d(s_space_dict[condition]['line_collect'], zs=z, zdir='z')\n",
    "\n",
    "    # for trial group data\n",
    "    for trial_lc in s_space_dict[condition]['line_collect_trial'].values():\n",
    "        \n",
    "        ax.add_collection3d(trial_lc, zs=z, zdir='z')\n",
    "        \n",
    "ax.legend(['All Trial Avg','Trial 1-10 Avg','Trial 11-20 Avg',\n",
    "           'Trial 21-30 Avg','Trial 31-40 Avg','Trial 41-50 Avg']);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
